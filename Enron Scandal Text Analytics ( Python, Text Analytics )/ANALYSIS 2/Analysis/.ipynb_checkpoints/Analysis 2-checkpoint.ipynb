{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analysis _ 2\n",
    "- For any give N no of article , we are find the relation between the section and most frewquent words of tht section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We are importing the CSV file and setting the limit \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords# We are importing the stopwords file package from nltk\n",
    "from operator import itemgetter # Importing the itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-1-f5c93d0dae86>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f5c93d0dae86>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    filepath =(os.path.join(os.getcwd(),'Data_processed\\\\Analysis - 2 _Data))\u001b[0m\n\u001b[0m                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "#We importing the necessary package \n",
    "filepath =(os.path.join(os.getcwd(),'Data_processed\\\\Analysis - 2 _Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We are defining the empty list and dictionary \n",
    "dictionaryinsidedict={}\n",
    "latestdic={}\n",
    "templist = []\n",
    "sectionlist = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop which is used to creat a main dictionary with the key words \n",
    "for name1 in os.listdir(filepath):# It will list all the directory \n",
    "        newd1= os.path.join(filepath,name1)# it will join two path \n",
    "        for file in glob.glob(os.path.join( newd1,'*.json')):# it will join two path \n",
    "                with open(file) as json_file:\n",
    "                    json_data = json.load(json_file) # reading the json data\n",
    "                    section = json_data[\"section_name\"] # we are find the value tagged to section name\n",
    "                    lead_paragraph = json_data[\"lead_paragraph\"]# we are find the value tagged to lead_paragraph\n",
    "                    if section not in sectionlist:\n",
    "                        wordlist =word_tokenize(lead_paragraph) # we are tokenized all the sentence to words \n",
    "                        sectionlist[section] = wordlist   # Adding to the dictionary if the words are not present \n",
    "                    else:\n",
    "                         sectionlist[section].extend(wordlist) # incrementing the count if the are present in the  dictionary \n",
    "\n",
    "\n",
    "                    \n",
    "                     \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop=stopwords.words('english')\n",
    "# we are using the stop words to clean the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# main loop used to find the top words for each unique section across all the article \n",
    "for key,value in sectionlist.items(): # we are getting the key and value of each item \n",
    "    for words in value:\n",
    "        words = words.lower()  # we are change the words to lower \n",
    "        if words not in stop: # we are checking if its not in stop words\n",
    "            if words.isalpha(): # we are checking if its alpha \n",
    "                if words not in dictionaryinsidedict:\n",
    "                    dictionaryinsidedict[words] = 1   # Adding to the dictionary if the words are not present \n",
    "                else:\n",
    "                     dictionaryinsidedict[words] += 1 # incrementing the count if the are present in the  dictionary \n",
    "    sorting = sorted(dictionaryinsidedict.items(), key=itemgetter(1),reverse=True) \n",
    "    dictionaryinsidedict={}\n",
    "    \n",
    "    for i in range(0,5): # we are taking the top 5 words for the section name  and appending in to the section value \n",
    "        templist.append(sorting[i][0])\n",
    "    latestdic[key]= templist  # assiging the \n",
    "    templist=[]\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we are creating the csv file to write the data into it \n",
    "\n",
    "with open (\"Analysis2.csv\",'a+') as outputfile1:\n",
    "    outputfile1.write(\"For the any given section in the artical \\n\\n\\n\")\n",
    "    outputfile1.write(\"Section Name's\"+\",\"+\"Top 5 words for that section\"+\"\\n\")\n",
    "\n",
    "new=\" -- \"\n",
    "for key , value in latestdic.items():\n",
    "    for words in value:\n",
    "        new = new+words+\" -- \" \n",
    "\n",
    "    with open (\"Analysis2.csv\",'a+') as outputfile1:\n",
    "        outputfile1.write(str(key)+\",\"+str(new)+\"\\n\")\n",
    "\n",
    "    \n",
    "    \n",
    "    new=\" -- \"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion\n",
    "- Based on the unique section in any give article , we can the find the relationship between the top most words present acros any given article for the given section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
